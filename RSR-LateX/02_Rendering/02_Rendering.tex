\newpage
\section{Rendering im Film}
\subsection{Einleitung}
\label{sec:introduction}
Ist der Charakter geriggt und geskinnt, muss er entsprechend seiner Eigenschaften korrekt gerendert werden. Rendering bezeichnet im Allgemeinen das Erstellen eines zweidimensionalen Bildes aus einer dreidimensionalen Szene oder einem Objekt unter Berücksichtigung physikalischer Größen wie Licht, Schatten und Materialeigenschaften \cite{renderingDefinition}. Dabei werden die Objekte in kleine Polygone wie zum Beispiel Dreiecke unterteilt, um die benötigten Operationen des Renderings auf einer einheitlichen Form durchzuführen. Bereits kleine Modelle können so aus mehreren tausend Polygonen bestehen. Rendering wird in einer Vielzahl von unterschiedlichen Bereichen angewandt. Zum einen in der Darstellung von Computerspielen, bei Konstruktionen in CAD-Programmen, aber auch bei CGI in aktuellen Filmen. 
Jeder Anwendungsfall hat andere Anforderungen und Prioritäten an das Rendering. Während Computerspiele vor allem ein schnelles Erzeugen von Bildern benötigen, kommt es in Filmen auf eine möglichst reale und detailreiche Darstellung an. Das folgende Kapitel beschäftigt sich mit den besonderen Anforderungen an das Rendering im Film. 

Hier müssen die Bilder in angemessener Zeit gerendert werden und trotzdem einen hohen Detailgrad aufweisen. Zusätzlich müssen sich Licht und Materialien im Filmkontext korrekt verhalten und dem Betrachter das Gefühl einer realistischen Szene geben. Das heißt, es müssen Reflexionen, Brechungen von Licht, Schatten und Materialeigenschaften wie Transparenz berücksichtigt werden. All das kann bereits bei relativ kleinen Objekten zu aufwendigen Berechnungen führen. Beim Film wird jedoch mit sehr komplexen Szenen gearbeitet, welche aus vielen Objekten bestehen. Aus diesen Szenen müssen für eine Sekunde Film 24 hochauflösende Bilder erstellt werden. Im Folgenden werden zunächst grundlegende Eigenschaften von Beleuchtung und Materialien erörtert, um dann unter Berücksichtigung dieser auf die Erzeugung einer komplett synthetischen Szene, wie in Abbildung \ref{cars}, als auch das Einfügen eines synthetischen Charakters in eine reale Szene, einzugehen.

\begin{figure}[t]
\centering
	\includegraphics[width=13cm]{02_Rendering/img/cars.png}
	\caption[Bild aus dem Film Cars]{Bild aus dem Film Cars. Entnommen aus \cite{cars}.}
	\label{cars}
\centering
\end{figure}

\subsection{Licht}
\label{light}
Jede Rendering-Technik versucht das gleiche physikalische Phänomen abzubilden: Die Streuung von Licht \cite{renderingEquation}.
Dabei ist zu beachten, dass sowohl spekulare sowie diffuse Reflexionen auftreten können. Eine ideal spekulare Reflexion kommt vor, wenn ein Lichtstrahl eine Spiegeloberfläche trifft. Hier wird nur ein einziger Strahl nach dem Gesetz \textit{Einfallswinkel ist gleich Ausfallswinkel} reflektiert. Ideal diffuse Reflexion tritt hingegen auf, wenn ein Lichstrahl auf eine ideal matte Oberfläche trifft und gleichmäßig in alle Richtungen gestreut wird. Wie viel Licht sich an einem Punkt im Raum befindet, ergibt sich zum einen aus dem direkt von einer Lichtquelle einfallenden Licht, zum anderen durch das Licht, dass von umliegenden Punkten auf den Punkt reflektiert wird. Der Vorgang, die passende Farbe für einen Punkt entsprechend der Lichteinstrahlung und des Materials zu finden, nennt sich Shading. Dieser Vorgang wird mit Hilfe lokaler Reflexionsmodelle beziehungsweise globaler Beleuchtungsmodelle realisiert. Reale Bilder benötigen fast immer eine Kombination der beiden Modelle.

\subsubsection{Lokale Reflexionsmodelle}
\label{localLight}
In der Realität verhalten sich verschiedene Materialien bei gleicher Beleuchtung unterschiedlich bezüglich ihrer Reflexionseigenschaften. Die Menge des reflektierten Lichts eines Punktes bei direkter Beleuchtung in eine bestimmte Richtung kann mittels einer Bi-Directional Reflection Distribution Function (BRDF) berechnet werden. Hierbei wird der Punkt völlig isoliert in der Szene betrachtet. Wechselwirkungen mit anderen Objekten, wie Schatten oder gegenseitige Reflexionen, werden im lokalen Modell nicht berücksichtig.  Alan Watt beschreibt diese Funktion in seinem Buch \textit{3D Computergrafik} wie folgt \cite{localLightCite}:
\begin{equation}
\label{BRDFEq}
\mathrm{BRDF}=f(\theta_\mathrm{in}, \phi_\mathrm{in}, \theta_\mathrm{ref}, \phi_\mathrm{ref}) = f(\vec{\omega_{i}}, \vec{\omega_{o}})
\end{equation}
Dabei beschreiben $\phi_\mathrm{in}$ beziehungsweise $\phi_\mathrm{ref}$ den Einfallswinkel sowie $\theta_\mathrm{in}$ und $\theta_\mathrm{ref}$ den Brechungswinkel der Strahlen. Die aus \cite{localLightCite} entnommene Abbildung \ref{BRDF} verdeutlicht die Beziehung der Parameter.
\begin{figure}[t]
\centering
	\includegraphics[width=13cm]{02_Rendering/img/brdf.jpg}
	\caption[BRDF]{BRDF zur Beschreibung des Verhältnisses des einfallenden Lichts $L$ und des reflektierten Lichts $V$. Entnommen aus \cite{localLightCite}.}
	\label{BRDF}
\centering
\end{figure}
Um die unterschiedlichen Reflexionseigenschaften von Materialien zu simulieren, werden verschiedene BRDFs modelliert. Die Kombination einzelner spekularer beziehungsweise diffuser Komponenten simuliert das Verhalten realer Oberflächen. Im Folgenden werden zwei Modelle zur Reflexion vorgestellt, wie sie in \cite{brdf, brdf2} beschrieben sind.

\begin{itemize}
	\item \textbf{Lambertsche Reflexion.} Bei der Lambertschen Reflexion geht man davon aus, dass eine beleuchtete Oberfläche bei gleicher Lichtintensität in jede Richtung mit der gleichen Intensität reflektiert. Sie ist damit gut zur Darstellung diffuser Reflexionen, also matter Oberflächen wie Papier, geeignet. Da die Lambertsche BRDF nur von der Intensität des einfallenden Lichts abhängt, kann sie ohne Berücksichtigung der Reflexionsrichtung berechnet werden. Die Intensität des reflektierten Lichts ergibt sich aus $I_{D} =\vec{\omega_{i}} \cdot \vec{n}CI_{L}$. Durch das Skalarprodukt der Richtung des eintreffenden Lichtstrahls $\vec{\omega_{i}}$ mit der Normalen der Fläche $\vec{n}$ geht bei einem steil eintreffenden Lichtstrahl ein höherer Wert der Farbe $C$ entsprechend der Lichtintensität $I_{L}$ in die Intensität des reflektierten Lichtstrahls $I_D$ ein. Ein mit einer Lambertschen BRDF erstelltes Dreieck ist in Abbildung \ref{lambert} zu sehen.
	
	\item \textbf{Blinn-Phong Modell.} Das Blinn-Phong Modell basiert auf der Lambertschen Reflexion. Jedoch wird hier ein optisches Glanzlicht hinzugefügt, wenn die Normale der Fläche etwa in der Mitte zwischen eingehenden und reflektierten Licht liegt. Es werden also diffuse und spekulare Reflexionen eingesetzt um zum Beispiel Metalle darzustellen. Die Blinn-Phong BRDF lässt sich wie folgt berechnen:
	
\begin{equation}
\label{blinnPhongEquation}
f_{s}(\vec{\omega_{i}}, \vec{\omega_{o}})=\frac{k_{L}}{\pi} + k_{G}\frac{8 + s}{8\pi}z^s \mathrm{\quad mit\quad}z=\max(0,\vec{h} \cdot \vec{n}) \mathrm{\quad mit\quad}\vec{h}=\frac{\vec{\omega_{i}} + \vec{\omega_o}}{2}
\end{equation}


$\vec{h}$ ist die Winkelhalbierende und befindet sich in der Mitte zwischen dem einfallenden Licht $\vec{\omega_{i}}$ und der ausgehenden Reflexion $\vec{\omega_o}$. Je kleiner der Winkel dieser Winkelhalbierenden und der Oberflächennormalen $\vec{n}$, ist desto größer wird der Wert $z$. Je höher $z$, desto stärker geht die Glanzkonstante $k_G$ in die Formel ein. $k_G$ kontrolliert die Intensität und Farbe des optischen Glanzlichts. Analog dazu bestimmt die Lambertsche Konstante $k_L$ die Intensität und Farbe der matten Stellen der Oberfläche. Der Glättegrad $s$ gibt an wie glatt die Oberfläche ist. Dabei sind niedrige Werte um die 60 für Leder oder mattes Plastik und hohe Werte um die 2000 für stark spekulare Oberflächen, wie zum Beispiel bei Autolacken oder Keramikoberflächen geeignet. Der Normalisierungsfaktor $\frac{8 + s}{8\pi}$ erhöht die Intensität der Glanzlichter für glatte Oberflächen. Die 8er entstehen durch die Rundung der Konstanten der Lösung des Integrals für den Glanzlichtterm über die gesamte Hemisphäre.
Ein mit der Blinn-Phong BRDF gerendertes Dreieck ist in Abbildung \ref{blinnPhong} zu sehen. 
\end{itemize}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{02_Rendering/img/lambertian.jpg}
  \caption[Lambertsches Dreieck]{Lambertsche BRDF}
  \label{lambert}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{02_Rendering/img/glossy.jpg}
  \caption[Blinn-Phong Dreieck]{Blinn-Phong BRDF}
  \label{blinnPhong}
\end{subfigure}
\caption{Vergleich von Lambertschen BRDF und einer Blinn-Phong BRDF.}
\label{vergleich}
\end{figure}

\subsubsection{Globale Beleuchtungsmodelle}
Im Gegensatz zu den lokalen Reflexionsmodellen berücksichtigen globale Beleuchtungsmodelle nicht nur Licht, welches direkt von einer Lichtquelle auf einen Punkt auftrifft, sondern auch alles Licht, das über Reflexionen anderer Objekte verteilt wird. Um dies zu realisieren muss Licht durch die gesamte Szene hinweg verfolgt werden und nicht nur von der Lichtquelle zu einem Punkt und dann zum Betrachter. Es ist leicht ersichtlich, dass die Berücksichtigung globaler Reflexionseffekte eine deutlich höhere Komplexität als die Berechnung lokaler Reflexionsmodelle aufweist. Einen mathematischen Ansatz zur Berechnung der Intensität der Beleuchtung an einem Punkt $x$ über einen anderen Punkt $x'$ veröffentlichte Kajiya 1986 in dem Artikel \textit{The Rendering Equation} \cite{renderingEquation}. Es handelt sich um eine völlig allgemeine Aussage über das Problem der globalen Beleuchtung. Die Rendering-Gleichung nach \cite{renderingEquation} ergibt sich aus:

\begin{equation}
\label{renderingEquation}
	L(x, x')=g(x, x') \cdot \lbrack L_e(x,x') + \int_{s} p(x,x',x'')L(x',x'')dx''\rbrack
\end{equation}

Die Lichtintensität am Punkt $x$ ergibt sich aus dem Licht, welches von $x'$ auf $x$ gestrahlt wird, $L_e(x,x')$, und dem Integral über alle Punkte aller Oberflächen der Szene $s$. $p(x,x' ,x'')$ nennt Kajiya den Drei-Punkt-Transport-Reflexionsgrad. Dieser entspricht der oben genannten BRDF, die die Reflexionseigenschaften für das Licht, welches von $x''$ über $x'$ zu $x$ gelangt, beschreibt. $L(x',x'')$ gibt hingegen die Intensität des Lichts von Punkt $x''$ zu $x'$ an. Der geometrische Term $g(x,x')$ dient der Berechnung der Sichtbarkeit der beiden Punkte und kann Werte zwischen $0$, wenn die Punkte sich gar nicht sehen, und $1$, wenn eine optimale Sichtbarkeit vorliegt, annehmen.
Mit Hilfe der Rendering-Gleichung kann also theoretisch die globale Beleuchtung eines jeden Punktes abhängig von allen anderen Punkten, berechnet werden. Dabei handelt es sich um ein blickwinkelunabhängiges Ergebnis. Das heißt, die Werte werden ohne Berücksichtigung der Position des Betrachters berechnet. Leider ist das Integral der Rendering Gleichung so komplex, dass es analytisch nicht gelöst werden kann. Es gibt aber Algorithmen, die die Komplexität reduzieren und an die Rendering-Gleichung angenäherte Ergebnisse liefern.

\subsection{Rendering-Techniken}
Wie bereits erwähnt ist die Rendering Gleichung analytisch nicht berechenbar. Da jedoch in der Realität Reflexionen zwischen Oberflächen einen Großteil der Beleuchtung einer Szene ausmachen, kann die globale Beleuchtung im Rendering nicht vernachlässigt werden. Gerade im Film wird sie zur realistischen Darstellung von Objekten benötigt. Eine professionelle Rendering-Technik muss jedoch noch einige weitere Fähigkeiten mitbringen. Catmull, Cook und Carpenter beschreiben diese in ihrem Artikel zum Pixar-eigenen Renderer Reyes wie folgt \cite{REYES}:

\begin{itemize}
	\item \textbf{Modellkomplexität.} In einem mit CGI versehenen Film sollen Bilder erzeugt werden, die visuell besonders ansprechend und detailreich sind. Das setzt voraus, dass Szenen mit einer Vielzahl von einzelnen Objekten, die wiederum unterteilt sein können, gerendert werden müssen.
	
	\item \textbf{Diversität der Modelle.} Die verwendete Rendering-Technik muss eine Vielzahl geometrischer Primitiven wie Dreiecke, Vierecke oder Partikelsysteme verarbeiten können.
	
	\item \textbf{Komplexes Shading.} Die Reflexionseigenschaften verschiedener Materialien und Oberflächen sind äußert unterschiedlich und komplex. Eine gute Rendering-Technik gibt dem Anwender die Möglichkeit, selbst einen Shader zu implementieren. Dabei muss es auch möglich sein, Textur- oder Umgebungs-Maps einzusetzen.
	
	\item \textbf{Geschwindigkeit.} Um einen Film mit einer Länge von zwei Stunden und 24 Bildern pro Sekunde in einem Jahr zu rendern, wird eine Rendering-Dauer von drei Minuten pro Bild benötigt. Rendering Farms und verteiltes Rendering ermöglichen ein schnelles Erstellen synthetischer Bilder, eine professionelle Rendering-Technik muss parallelisierbar sein und gerade für komplexe Szenen hoch performant funktionieren.
	
	\item \textbf{Bildqualität.} Die gerenderten Bilder müssen möglichst real erscheinen und dürfen zum Beispiel keine Treppenartefakte aufweisen.
	
	\item \textbf{Flexibilität.} Im Laufe der Zeit werden immer neue und bessere Rendering-Methoden entwickelt. Eine aktuelle Rendering-Technik sollte flexibel genug sein, neue Methoden in den bestehenden Rendering-Prozess zu integrieren.
\end{itemize}
Im Folgenden werden kurz zwei Rendering-Techniken besprochen, welche eine Annäherung an die durch die Rendering-Gleichung beschriebene globale Beleuchtung ermöglichen und die oben genannten Anforderungen erfüllen. Die Techniken werden bereits zur Erzeugung synthetischer Filme eingesetzt und sind in den von Pixar Animation Studios bereitgestellten Artikeln \cite{cars} und \cite{REYES} beschrieben.

\subsubsection{Die Reyes-Bild-Rendering-Architektur}
Reyes ist eine bei Pixar Animation Studios eingesetztes Rendering-Technik. Sie stellt eine schnelle und hochqualitative Lösung zum Rendern komplexer Bilder dar. Hierbei bezieht sich schnell darauf, dass ein Spielfilm in etwa einem Jahr gerendert werden kann. Hochqualitativ bedeutet, dass synthetisch erzeugte Bilder nicht von realen Fotos zu unterscheiden sind und komplex, dass Reyes auf grafisch sehr aufwendige Szenen angewendet werden kann.

Dies gelingt Reyes, da es immer nur kleine Teile der Szene im Hauptspeicher halten muss. Dieses Prinzip wird auch geometrische Lokalität genannt. Berechnungen geometrischer Primitiven werden ohne Abhängigkeit anderer geometrischer Primitiven im Raum durchgeführt. In vielen Fällen können globale Berechnungen durch Textur-Maps angenähert werden. Reflexionen können zum Beispiel durch Umgebungs-Maps und Schatten durch Schatten-Maps dargestellt werden. Beim Mapping-Vorgang werden die Punkte der Map im Texturraum den Punkten des Polygons zugeordnet. Umgebungs- und Schatten-Maps können vor dem eigentlichen Rendering Prozess berechnet werden und bei gleichbleibenden Blickwinkel beliebig oft wiederverwendet werden. Ein tieferer Einblick in Textur-Mapping ist in \cite{maps} zu finden. 
Rendering-Techniken wie Raytracing oder Radiosity müssen hingegen immer Zugriff auf die gesamte Szene zum Shaden eines einzelnen Punktes haben. Bei diesen Techniken muss ein Lichtstrahl auf seiner gesamten Ausbreitung verfolgt werden und kann nicht im Voraus berechnet werden. Hoch komplexe Szenen können jedoch in vielen Fällen nicht als ganzes im Hauptspeicher gehalten werden. Aus diesem Grund kam Raytracing zum Rendern von Filmen lange nicht in Frage.

Da der Algorithmus von Reyes immer nur ein Objekt nacheinander einliest, tritt dieses Speicherproblem nicht auf.
Der Algorithmus für jedes Objekt ist in Abbildung \ref{reyesAlgo} beschrieben.
\begin{figure}[t]
\centering
	\includegraphics[width=.5\linewidth]{02_Rendering/img/reyesalgo.png}
	\caption[Reyes Rendering Algorithmus]{ Schritte im Reyes Renderer zum Rendern eines Objektes. Entnommen aus \cite{REYES}.}
	\label{reyesAlgo}
\centering
\end{figure}
Vor Beginn wird ein Z-Buffer für die gesamte Szene benötigt. So kann die Sichtbarkeit der Objekte überprüft werden.
Nach dem Einlesen wird jedes Objekt auf eine einheitliche, geometrische Form reduziert. Diese Form wird Mikropolygone genannt. Alle Shading-Operationen finden auf Basis dieser kleinsten Einheit statt. Die Aufteilung in Mikropolygone wird Dicing genannt. Es werden jedoch nur Objekte gediced, die sich komplett im sichtbaren Bereich befinden. Objekte, welche als Ganzes nicht sichtbar sind, werden verworfen. Befindet sich jedoch ein Objekt nur zum Teil im Sichtfeld des Betrachters, findet vor dem Dicing eine gröbere Unterteilung statt, welche sich Splitting nennt. Das Splitting hat als Ergebnis immer Patches, die sich auf den Texturraum mappen lassen. Die Patches werden dann erneut nach ihrer Diceability überprüft. Erst wenn ein Patch komplett sichtbar ist, wird er gediced. Nicht sichtbare Patches werden verworfen. Sind alle sichtbaren Teile eines Objektes in Mikropolygone aufgeteilt, so findet das Shading in Form von Textur-Mapping statt. Alle Mikropolygone innerhalb eines Patches werden auf einmal geshaded. Nach einem Antialiasing-Prozess wird das Bild erzeugt. In Abbildung \ref{sphere} ist eine Kugel zu sehen, welche den Algorithmus bis zum Dicing durchlaufen ist.

Die Flexibilität der Reyes-Rendering-Technik ist über eine Back Door gegeben, welche es dem Anwender ermöglicht, auch eigene Shader in den Rendering-Prozess zu integrieren.

\begin{figure}[t]
\centering
	\includegraphics[width=.5\linewidth]{02_Rendering/img/sphere.png}
	\caption[Spliting und Dicing am Beispiel einer Kugel]{ Spliting und Dicing am Beispiel einer Kugel. Entnommen aus \cite{REYES}.}
	\label{sphere}
\centering
\end{figure}

Die Reyes-Rendering-Technik bringt viele Vorteile mit sich. Das Shading findet auf simplen einheitlichen Formen statt. Es muss immer nur ein kleiner Teil der Szene im Hauptspeicher gehalten werden, so können nahezu beliebig komplexe Szenen gerendert werden. Durch das Anwenden von Textur-Maps können reelle Materialien und Reflexionseigenschaften mit einem vertretbaren Aufwand berechnet werden und Berechnungen wie Clipping entfallen.

\subsubsection{Raytracing}
 Wie bereits erwähnt, kam das Rendern komplexer Szenen mit einem Raytracer lange nicht in Frage. Christensen, Fong, Laur und Batali veröffentlichten jedoch 2006 ein Paper, in welchem sie beschreiben, mit welchen Techniken sie in der Lage waren, den im gleichen Jahr erschienenen Spielfilm Cars mit Raytracing zu verwirklichen \cite{cars}. Sie sehen die Vorteile im Raytracing gegenüber der Reyes-Rendering-Technik in der Darstellung korrekter Reflexionen und detailreicher Schatten. Objekte, deren Reflexionen über Umgebungs-Maps realisiert werden, können sich zum Beispiel nicht in sich selbst spiegeln oder Objekte reflektieren, welche auch eine Spiegeloberfläche besitzen. Eine Gegenüberstellung einer Reflexion, die mit einer Umgebungs-Map, und einer, die mit einem Raytracer gerendert wurde, ist in Abbildung \ref{reflection} zu sehen.
 
 \begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{02_Rendering/img/map.png}
  \caption[Reflexion durch Umgebungs-Map ]{Reflexion durch Umgebungs-Map.}
  \label{environmentMap}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{02_Rendering/img/traced.png}
  \caption[Reflexionen durch Ray Tracer]{Reflexion durch Raytracer.}
  \label{rayTracerReflect}
\end{subfigure}
\caption[Vergleich von Reflexionen erstellt durch Umgebungs-Maps beziehungsweise einem Raytracer.]{Vergleich von Reflexionen erstellt durch Umgebungs-Maps beziehungsweise einem Raytracer. Entnommen aus \cite{cars}.}
\label{reflection}
\end{figure}

Um zu verstehen, inwieweit sich das Raytracing in einem Spielfilm wie Cars von einem gewöhnlichen Raytracer unterscheidet, wird zunächst die Vorgehensweise eines herkömmlichen rekursiven Raytracers nach \cite{whitted} beschrieben, um dann die leistungssteigernden Anpassung des Raytracers aus \cite{cars} zu beleuchten.

Das Raytracing verfolgt Lichtstrahlen entgegengesetzt der Richtung ihrer Fortpflanzung vom Auge zurück in die Szene und zur Lichtquelle. Zur Erstellung eines zweidimensionalen Bildes sind nur die Lichtstrahlen interessant, welche auch wirklich in das Auge des Betrachters fallen. Es wird für jedes Pixel des zu rendernden Bildes ein Strahl in die Szene geworfen. Wird ein Objekt getroffen, wird ein reflektierter beziehungsweise bei transparenten Objekten ein gebrochener Strahl am Schnittpunkt erzeugt und rekursiv verfolgt. Da immer nur ein oder zwei Strahlen pro Schnittpunkt erzeugt werden können, lassen sich mit einem rekursiven Raytracer nur spekulare Wechselwirkungen modellieren. Der Algorithmus kommt zum Stillstand, wenn der Strahl auf eine diffuse Oberfläche oder die Lichtquelle trifft,  die Szene verlässt oder eine Maximalanzahl an Schnittpunkten erreicht hat. Letzteres ist zum Beispiel notwendig, wenn ein Strahl immer wieder von einem Spiegel in einen anderen reflektiert wird. Schatten werden generiert, indem bei jedem Schnittpunkt deren Sichtbarkeit von allen Lichtquellen aus geprüft wird.

Für die Schnittpunktberechnung eines jeden Strahls ist eine Iteration über alle Polygone der Szene notwendig. Zusätzlich müssen alle Polygone bei der Berechnung der Schatten berücksichtigt werden. Das macht die Schnittpunktberechnung enorm aufwendig und setzt bereits bei kleinen Szenen eine hohe Rechenleistung und einen großen Hauptspeicher voraus.

Der hohe Aufwand des Raytracings wird also hauptsächlich durch die Schnittpunktberechnung verursacht. Um Raytracing  zum Rendern komplexer Szenen einzusetzen, können einige Optimierungen durchgeführt werden. Alle Objekte werden in einfache Körper eingeschlossen, die als Hüllkörper bezeichnet werden. Nun wird zuerst geprüft, ob ein Strahl den Hüllkörper schneidet, bevor der tatsächliche Schnittpunkt mit dem Objekt berechnet wird. Hüllkörper, wie zum Beispiel eine Kugel zeichnen sich durch die leichte Berechnung von Schnittpunkten und durch ein möglichst knappes Umhüllen von Objekten aus. Um den Nutzen von Hüllkörpern zu maximieren, können mehrere Objekte auch zu Gruppen zusammengefasst und umschlossen werden. Die Erzeugung und der effiziente Einsatz von Hüllkörpern wird in \cite{boundingVolume} beschrieben. In diesem Modell, werden auch zunächst die Hüllkörper für die Schnittpunktberechnung betrachtet, welche dem Strahl am nächsten sind. Eine zweite Möglichkeit zur Optimierung der Schnittpunktberechnung ist die Anwendung von Strahlenkohärenz. Dabei macht man sich die Ähnlichkeit von Nachbarstrahlen zunutze. Diese neigen dazu, anfangs den gleichen Weg zu beschreiben. Bei der Strahlenkohärenz werden anstatt einzelner nur noch Bündel von Strahlen verfolgt und deren Schnittpunkt gemeinsam berechnet. Zunächst entspricht ein Strahlenbündel dem Ansichtsstumpf. In der Regel schneidet dieses Strahlenbündel jedoch mehrere Objekte. Für jeden dieser Schnittpunkte wird ein neues Strahlenbündel erzeugt. So gruppiert man nach und nach diejenigen Strahlen, die einen sehr ähnlichen Weg beschreiben. 
Für den Film Cars wurde ein Raytracer mit unter anderem oben genannten Modifikationen mit dem Reyes-Renderer kombiniert.

\subsection{Rendern synthetischer Objekte in eine echte Szene}
Bis jetzt wurden in diesem Kapitel Grundlagen für das Verhalten von Licht auf unterschiedlichen Materialien unter Berücksichtigung von direkter und indirekter Beleuchtung besprochen. Außerdem wurden zwei Techniken zum Erstellen komplett synthetischer Bilder und deren Anpassungen zum Rendern komplexer Szenen vorgestellt. In dem nun folgenden Teil soll dieses Wissen zum Rendern eines synthetischen Objektes in eine echte Szene eingesetzt werden. Ziel ist es den beim Riggen und Skinnen erzeugten Charakter in eine Szene einzufügen mit korrekten Schatten zu versehen und die Sichtbarkeit anhand der Position in der Szene zu bestimmen. Die hier angewandte Vorgehensweise entspricht dem in \cite{realImages} vorgestellten Algorithmus.

\subsubsection{Distanzszene, lokale Szene und synthetisches Modell}
Damit sich die synthetische Figur innerhalb der Szene korrekt verhält, müssen die möglichen Wechselwirkungen zwischen ihr und der realen Szene berücksichtigt werden. Die eingefügte Figur kann Schatten auf die reale Szene werfen, in Reflexionen auftauchen, Licht brechen oder es sogar aussenden. Um diese Interaktion zwischen dem Bild und dem Modell zu ermitteln, wird die gesamte Szene zunächst in drei Teile aufgeteilt.

\begin{itemize}
\item \textbf{Ein lichtbasiertes Modell der Distanzszene.} Die Distanzszene besteht aus dem Teil der Szene, welcher nicht signifikant durch die Reflexionseigenschaften des eingefügten synthetischen Modells beeinflusst wird. Die Distanzszene selbst kann aber sehr wohl einen Effekt auf das eingefügte Modell haben. Sie ist hauptverantwortlich für die korrekte Beleuchtung im finalen Bild und muss deshalb präzise Informationen über die Intensität des ausgesandten Lichts an jedem Pixel besitzen. Es ist äußert schwer, die genauen Werte der Intensität in einer realen Szene zu bestimmen. Fotografien oder Einzelbilder in einem Video haben einen sehr großen Umfang an möglichen Lichtintensitäten. Direkte Lichtquellen weisen sehr hohe Intensität auf, wohingegen diffuse Oberflächen nahezu kein Licht emittieren. Wie bereits im Kapitel über \nameref{light} erwähnt, sind aber beide Bereiche zur Berechnung einer glaubhaften Beleuchtung notwendig. Durch die in \cite{radianceMaps} vorgestellten Techniken kann die korrekte Lichtintensität einer realen Szene festgestellt werden. Dazu werden mehrere Bilder mit unterschiedlichen Belichtungszeiten geschossen und zur Berechnung eines High Dynamic Range Image(HDRI) herangezogen. Dieser Vorgang ist in Abbildung \ref{shutter} mit einigen beispielhaften Belichtungszeiten dargestellt.

\item \textbf{Ein geschätztes materialbasiertes Modell der lokalen Szene.} Die lokale Szene besteht aus allen Oberflächen, die fotometrisch mit dem künstlichen Modell interagieren. Das synthetische Modell reflektiert Licht oder wirft Schatten auf die lokale Szene. Diese emittiert im Gegensatz zur Distanzszene also nicht nur Licht, sondern muss auch mit passenden Reflexionseigenschaften beziehungsweise BRDFs versehen werden. Nur so können die Wechselwirkungen mit dem eingefügten Modell korrekt modelliert werden. Im Laufe des Rendering-Prozesses wird versucht, eine dreidimensionale Abbildung der lokalen Szene zu ermitteln. Häufig passiert dies manuell oder auch automatisiert wie in \cite{commonIllumination} beschrieben. In vielen Fällen ist die lokale Szene eine rein diffuse und flache Oberfläche - zum Beispiel, wenn es sich um den Boden eines Raumes oder die Oberfläche eines Holztisches handelt. Unter diesen Bedingungen sind die Wechselwirkungen zwischen synthetischen Model und lokaler Szene häufig gering. Jedoch gibt es auch einige Umstände, unter denen ein künstliches Objekt eine starke Wirkung sogar auf weiter entfernte Teile der Szene haben kann. Diese sind:

\begin{itemize}
	\item Eine konzentrierte Lichtquelle bestrahlt das synthetische Modell, sodass es einen starken Schatten wirft.
	\item Eine konzentrierte Lichtquelle bestrahlt das synthetische Model, welches viele spekulare Oberflächen besitzt. So kann Licht bis in weit entfernte Teile der Szene reflektiert werden.
	\item Handelt es sich bei der Distanzszene um spekulare Oberflächen wie ein Spiegel, können diese vom synthetischen Modell beeinflusst werden.
	\item Ein Licht emittierendes synthetisches Modell kann Lichtstrahlen bis in die Distanzszene tragen. 
\end{itemize}

Diese Situationen müssen bei der Aufteilung der Szene in Distanzszene und lokale Szene berücksichtigt werden. Hat das künstlich eingefügte Objekt in irgendeiner Form einen Einfluss auf eine Oberfläche, so sollte diese der lokalen Szene zugeordnet werden.

\item \textbf{Künstliches Objekt mit bekannten Materialeigenschaften.} Dieser Teil der Szene beinhaltet das Modell beziehungsweise die Figur, die im finalen Bild nahtlos in die reale Szene eingebettet werden soll.  
\end{itemize} 
Die Abhängigkeiten und Wechselwirkungen der Distanzszene, lokalen Szene und dem künstlichen Objekt sind in Abbildung \ref{scenes} dargestellt.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{02_Rendering/img/room_1_2.JPG}
  \caption[Aufnahme mit 0.5 Sekunden Belichtungszeit.]{Aufnahme mit 0.5 Sekunden Belichtungszeit.}
  \label{shutterhalf}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{02_Rendering/img/room_1.JPG}
  \caption[Aufnahme mit 1 Sekunde Belichtungszeit.]{Aufnahme mit 1 Sekunde Belichtungszeit.}
  \label{shutterone}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{02_Rendering/img/room_4.JPG}
  \caption[Aufnahme mit 4 Sekunde Belichtungszeit.]{Aufnahme mit 4 Sekunde Belichtungszeit.}
  \label{shutterone}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.4\linewidth]{02_Rendering/img/room.png}
  \caption[Erstelltes HDRI]{Erstelltes HDRI}
  \label{shutterHDRI}
\end{subfigure}
\caption[Erstellung eines HDRI aus mehreren Bildern unterschiedlicher Belichtungszeit.]{Erstellung eines HDRI aus mehreren Bildern unterschiedlicher Belichtungszeit.}
\label{shutter}
\end{figure}

\begin{figure}[t]
\centering
	\includegraphics[width=.5\linewidth]{02_Rendering/img/scenes.png}
	\caption[Wechselwirkungen der Distanzszene, lokalen Szene und eines synthetischen Objekts.]{ Wechselwirkungen der Distanzszene, lokalen Szene und eines synthetischen Objekts. Entnommen aus \cite{realImages}.}
	\label{scenes}
\centering
\end{figure}

\subsubsection{Beleuchtung des synthetischen Modells}
Die möglichst realitätsgetreue Einbettung des synthetischen Modells steht und fällt mit der korrekten Beleuchtung. Das Modell muss sich entsprechend der Position der Lichtquellen in der realen Szene korrekt verhalten und Licht reflektieren, gegebenenfalls Glanzlichter aufweisen oder Schatten werfen. Unter der Annahme, dass die Szene bereits in die drei Bestandteile Distanzszene, lokale Szene und synthetisches Objekt aufgeteilt ist, kommen einige Ansätze zur Illumination in Frage. Der folgende Abschnitt beschäftigt sich mit einer Alternative zum manuellen Setzen von Lichtquellen. Dazu wird eine Lichtsonde benötigt. Lichtsonden beinhalten die Beleuchtungsinformationen für die sie umgebende Szene. Häufig wird eine Lichtsonde in Form eines Spiegelballes verwendet. Dieser wird im Bild ungefähr an die Stelle gelegt, an der sich das synthetische Modell befinden soll. In jedem Falle müssen aber die Materialeigenschaften und Größe sowie die Position des Balles relativ zur Position der Kamera bekannt sein. Nun werden zwei HDR-Bilder, mit einem Blickwinkelunterschied von 90 Grad, von der Szene mit dem Spiegelball angefertigt. Durch die 90-Grad-Verschiebung der beiden Bilder ist sicher gestellt, dass die sich in der Kugel reflektierende Umgebung einen Rundblick der ganzen Szene beinhaltet. Außerdem würden Bilder von nur einem Blickwinkel Artefakte unumgänglich machen. Die Reflexion im Spiegelball würde die Kamera oder sogar den Kameramann beinhalten. Zwei Bilder aus unterschiedlichen Winkeln können sich über korrespondierende Punkte auf der Kugeloberfläche ergänzen und so die besagten Artefakte eliminieren. Ein Beispiel, wie aus zwei Bildern eine Lichtsonde erstellt wird, ist in Abbildung \ref{probe} zu sehen.

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{02_Rendering/img/prob1.jpg}
  \caption[Spiegelball aus Perspektive 1]{Spiegelball aus Perspektive 1.}
  \label{probe1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{02_Rendering/img/prob2.jpg}
  \caption[Spiegelball aus Perspektive 2]{Spiegelball aus Perspektive 2.}
  \label{probe2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{02_Rendering/img/propfull.jpg}
  \caption[Zusammengesetzte Lichtsonde]{Zusammengesetzte Lichtsonde}
  \label{probefull}
\end{subfigure}
\caption[Bilder zur Erstellung einer Lichtsonde]{Die Abbildungen \ref{probe1} und \ref{probe2} weisen noch Artefakte auf. Der Kameramann sowie die Kamera ist zu sehen. In der Lichtsonde in \ref{probefull} sind diese Artefakte hingegen nicht mehr zu erkennen. Entnommen aus \cite{lightprobe}.}
\label{probe}
\end{figure}

Mit der Reflexion der Umgebung in der Lichtsonde und dem Wissen über ihre Position, ihrer Beschaffenheit und der relativen Position der Kamera kann ein dreidimensionales Objekt der Lichtsonde erstellt und korrekt in einer Box platziert werden. Nun kann das lichtbasierte Modell der Distanzszene erstellt werden. Dazu wird die Oberfläche der Kugel auf besagte Box gemapped. Strahlen werden vom Betrachter auf die synthetische Lichtsonde geschickt und der reflektierte Strahl verfolgt. Trifft der Strahl eine Wand der Box, so erhält der Punkt das Shading der Kugel. Führt man das für alle Punkte der Lichtsonde aus, erhält man eine Box, deren Wände einem lichtbasiertes Modell der Szene entsprechen. Jeder Punkt auf der Oberfläche der Box beinhaltet die Intensität, mit welcher die Lichtsonde mit dem korrespondierenden Punkt im realen Bild bestrahlt wurde. In Abbildung \ref{distantszene} ist das erstellte lichtbasierte Modell der Distanzszene abgebildet. Mit einer korrekt erstellten Distanzszene können synthetische Modelle entsprechend der Lichtbedingungen der realen Szene beleuchtet werden. Wichtig ist, dass sich das Modell ungefähr an der Stelle der Lichtsonde befindet und die aktuelle Kameraposition in Relation zur Kameraposition bei der Erstellung der Lichtsonde bekannt ist.

\begin{figure}[h]
\centering
	\includegraphics[width=.9\linewidth]{02_Rendering/img/distantszene.png}
	\caption[Beispiel eines lichtbasierten Modells.]{ Beispiel eines lichtbasierten Modells. Das Modell beinhaltet die komplette dynamische Breite der Intensitäten der realen Szene. Entnommen aus \cite{realImages}.}
	\label{distantszene}
\centering
\end{figure}

\subsubsection{Einfügen des synthetischen Modells in die Szene}
Um das synthetische Modell nahtlos in die reale Szene einzufügen, muss es in Kombination mit einem Bild der realen Szene gerendert werden. Dazu muss das dreidimensionale Abbild der lokalen Szene mit den korrekten BRDFs existieren. Die BRDFs können entweder manuell angegeben werden, oder über einen iterativen Prozess ermittelt werden. Hierbei wird das synthetische Abbild der lokalen Szene immer wieder mit unterschiedlichen Parametern für die BRDFs gerendert und das Ergebnis mit dem Bild der realen Szene verglichen.
Nun wird das synthetische Modell mit der lokalen Szene in das Bild der realen Szene eingefügt. Es können Bilder aus beliebigen Blickwinkeln der Szene gemacht werden, sofern die relative Position der ursprünglichen Lichtsonde für den Blickwinkel bekannt ist. Zum Rendern wird ein Raytracer angewendet. Die Strahlen werden in die Szene geschossen und verfolgt. Trifft der Strahl Teile der realen Szene beziehungsweise die Distanzszene, wird er verworfen. Trifft er jedoch Oberflächen des synthetischen Modells oder der lokalen Szene, so wird er rekursiv weiterverfolgt. Dieser Prozess wird gestoppt, nachdem eine maximale Anzahl an Schritten erreicht ist oder der Strahl in das lichtbasierte Modell der Distanzszene fällt. Die Intensität des Schnittpunktes geht in das Shading der lokalen Szene und des synthetischen Modells ein. Ein Nachteil bei diesem Vorgehen ist, dass die Materialeigenschaften sowie die Geometrie der lokalen Szene korrekt sein müssen. Wenn die BRDFs falsch gewählt wurden, hebt sich die gerenderte lokale Szene stark innerhalb des fertig gerenderten Bildes von dem unveränderten Teil der Szene ab. Für ein besseres Ergebnis sollten also nur die Teile der lokalen Szene in das final gerenderte Bild eingehen, welche durch das Einfügen des synthetischen Modells auch eine Beleuchtungsänderung erfahren haben.

\begin{equation}
\label{errorImage}
	\mathrm{LS_{final}} = \mathrm{LS_b} + (\mathrm{LS_{obj}} - \mathrm{LS_{noobj}})
\end{equation}

In dieser Formel kann man gut erkennen, dass wenn die Differenz der lokale Szene mit dem synthetischen Objekt, $\mathrm{LS_{obj}}$ und die lokale Szene ohne Objekt, $\mathrm{LS_{noobj}}$, $0$ ergibt die, finale lokale Szene, $\mathrm{LS_{final}}$ dem realen Bild der lokalen Szene, $\mathrm{LS_b}$ entspricht. Wenn $\mathrm{LS_{obj}}$ dunkler als $\mathrm{LS_{noobj}}$ ist, werfen die Objekte Schatten. Ist der Unterschied der beiden lokalen Szenen positiv, wird Licht von den Objekten auf die lokale Szene geworfen. Mit diesem Wissen, können nur noch die Teile der lokalen Szene beim Erstellen des finalen Bildes betrachtet werden, welche auch wirklich durch das synthetische Modell beeinflusst werden.

Durch den in diesem Abschnitt beschriebenen Algorithmus kann ein synthetisches Modell in eine reale Szene eingefügt werden. Das Modellieren  der lokalen Szene sorgt dafür, dass es korrekte Überschneidungen mit den realen Elementen der Szene gibt und das lichtbasierte Modell der Distanzszene liefert die nötigen Informationen über die ursprüngliche Beleuchtungssituation.

Die hier beschriebene Vorgehensweise eignet sich auch gut, um unseren synthetischen Charakter in ein reales Bild einzufügen. Sowohl das lichtbasierte Modell der Distanzszene als auch die Geometrie- und Materialeigenschaften der lokalen Szene müssen nur einmal bestimmt werden. Von da an ist nur noch die Position der Kamera relevant, um den synthetischen Charakter sowie die lokale Szene korrekt zu rendern. Ein Beispiel für das beschriebene Verfahren ist in Abbildung \ref{finalSzene} zu sehen. 


\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{02_Rendering/img/calibration.png}
  \caption[Kamera Kalibrierung und Lichtsonde]{Kamera Kalibrierung und Lichtsonde}
  \label{calibration}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{02_Rendering/img/localszene.png}
  \caption[Objekte und lokale Szene in der realen Szene]{Objekte und lokale Szene in der realen Szene}
  \label{localszeneObject}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{02_Rendering/img/final.png}
  \caption[Finales Ergebnis mit den eingefügten synthetischen Objekten]{Finales Ergebnis mit den eingefügten synthetischen Objekten.}
  \label{szenecomplete}
\end{subfigure}
\caption[Zwischenschritte vom Kalibrieren der Kamera bis hin zum finalen Bild mit synthetischen Objekten]{ Zwischenschritte vom kalibrieren der Kamera bis hin zum finalen Bild mit den eingefügten synthetischen Objekten unter Berücksichtigung der Beleuchtung. Entnommen aus \cite{realImages}}
\label{finalSzene}
\end{figure}